{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfsqFK384QI_"
      },
      "source": [
        "In this notebook, you should implement an exciting task, write a caption for images with an intelligent agent! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EedZ5qvW4QJF"
      },
      "source": [
        "We use the [COCO dataset](https://cocodataset.org/) for this purpose. COCO is large-scale object detection, segmentation, and captioning dataset. Also, we use the pycocotools library for some data-related works. So, you should install it first. Maybe it needs some dependencies that you have not on your PC. So, we recommend running this notebook on Google collab. You should upload data_related.py in the content folder on Colab if you want to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bM-Bt8F4QJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "927ba9a4-aa82-49e6-a00b-e50741450e4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (2.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pycocotools) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.0->pycocotools) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pycocotools\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFyODDm-4QJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2cad0e3-40dd-4d18-e15e-2c692bbcda6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cocoapi'...\n",
            "remote: Enumerating objects: 975, done.\u001b[K\n",
            "remote: Total 975 (delta 0), reused 0 (delta 0), pack-reused 975\u001b[K\n",
            "Receiving objects: 100% (975/975), 11.72 MiB | 28.05 MiB/s, done.\n",
            "Resolving deltas: 100% (576/576), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs('opt' , exist_ok=True)\n",
        "os.chdir('opt')\n",
        "!git clone 'https://github.com/cocodataset/cocoapi.git'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e56eRuut8qdM"
      },
      "source": [
        "The following command imports some data-related functions, and it takes about 10 minutes for running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v10C2fSh4QJL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "0a211a2d-9f2a-4996-832d-3ab3c6c4a561"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-aca05d367ec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdata_related\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_related'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import data_related"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhP_EYZ-4QJM"
      },
      "source": [
        "Your network should have two parts, a CNN for understanding the image and an LSTM for generating related sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOPIiotM4QJN"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P5NMaB14QJP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46wOlClL4QJQ"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        # todo: Define a CNN with an extended fully-connected. Your output should be of the shape Batch_Size x embed_size.\n",
        "        # Make sure that your model is strong enough to encode the image properly.\n",
        "        #######################\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad_(False)\n",
        "        \n",
        "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "        #######################\n",
        "    \n",
        "    def forward(self, images):\n",
        "        features = None\n",
        "        #######################\n",
        "        features = self.resnet(images)        \n",
        "        features = self.embed(features.view(features.size(0), -1))\n",
        "        #######################\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJYsAFRh4QJR"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size \n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "        # todo: Define an embedding layer to transform inputs from \"vocab_size\" dim to \"embed size\" dim.\n",
        "        #######################\n",
        "        self.embedding_layer = nn.Embedding(self.vocab_size, self.embed_size)\n",
        "        #######################\n",
        "        \n",
        "        # todo: Define an LSTM decoder with input size, hidden size, and num layers specified in the input.  \n",
        "        #######################\n",
        "        self.lstm  = nn.LSTM(input_size = self.embed_size, hidden_size = self.hidden_size, num_layers = self.num_layers, batch_first=True)\n",
        "        #######################\n",
        "        \n",
        "        # todo: Define a fully-connected layer to transform the output hidden size of LSTM to a \"vocab_size\" dim vector.\n",
        "        #######################\n",
        "        self.fc = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "        #######################\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return ( torch.zeros( self.num_layers , batch_size , self.hidden_size  ).to(device),\n",
        "        torch.zeros( self.num_layers , batch_size , self.hidden_size  ).to(device) )\n",
        "    \n",
        "    def forward(self, features, captions):            \n",
        "        captions = captions[:,  :-1]      \n",
        "        self.batch_size = features.shape[0]\n",
        "        self.hidden = self.init_hidden( self.batch_size )\n",
        "        outputs = None  \n",
        "        \n",
        "        # todo: Compute the output of the model.\n",
        "        #######################\n",
        "        embedings = self.embedding_layer(captions)\n",
        "        inputs = torch.cat((features.unsqueeze(1), embedings), 1)      \n",
        "        outputs, self.hidden = self.lstm(inputs, self.hidden)      \n",
        "        outputs = self.fc(outputs)\n",
        "        #######################\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def generate(self, inputs, max_len=20):\n",
        "        final_output = []\n",
        "        batch_size = inputs.shape[0]         \n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        max_sent_length = 20\n",
        "    \n",
        "        # todo: You should pass hidden state and previous vocab to LSTM successively, and stop generating when\n",
        "        # The length of the sentence exceeds max_sent_length, or EOS token (end of sentence, index 1) occurs.\n",
        "        # Just return indexes in final_output.\n",
        "        #######################\n",
        "        while True:\n",
        "            outputs, hidden = self.lstm(inputs, hidden) \n",
        "            outputs = self.fc(outputs)  \n",
        "            outputs = outputs.squeeze(1) \n",
        "            _, index = torch.max(outputs, dim=1) \n",
        "            final_output.append(index.cpu().numpy()[0].item())             \n",
        "            if index == 1:\n",
        "                break\n",
        "            elif len(final_output) >= max_sent_length:\n",
        "              break\n",
        "            inputs = self.embedding_layer(index).unsqueeze(1)\n",
        "        #######################\n",
        "        return final_output  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMh14sUX4QJS"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtTLR4o84QJS"
      },
      "outputs": [],
      "source": [
        "embed_size = 256\n",
        "hidden_size = 100\n",
        "num_layers =1 \n",
        "num_epochs = 4\n",
        "print_every = 150\n",
        "save_every = 1\n",
        "vocab_size = len(data_related.data_loader_train.dataset.vocab)\n",
        "total_step = math.ceil(len(data_related.data_loader_train.dataset.caption_lengths) / \n",
        "                       data_related.data_loader_train.batch_sampler.batch_size)\n",
        "lr = 0.001\n",
        "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_save_path = 'model_weights/'\n",
        "os.makedirs( model_save_path , exist_ok=True)\n",
        "\n",
        "encoder = Encoder(embed_size)\n",
        "decoder = Decoder(embed_size, hidden_size, vocab_size ,num_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkGzXXn-0bpi"
      },
      "outputs": [],
      "source": [
        "# todo: Define loss function and optimizer for encoder and decoder weights.\n",
        "#######################\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params = list(decoder.parameters()).extend(list(encoder.embed.parameters())), lr = lr)\n",
        "####################### "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKda9Iwz8qdS"
      },
      "source": [
        "The training process may take up to 10 hours. Save the model frequently. If the training process stops for some reason, continue from the last saved model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MetWL4204QJS"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "for e in range(1, num_epochs):\n",
        "    for step in range(total_step):\n",
        "        indices = data_related.data_loader_train.dataset.get_train_indices()\n",
        "        new_sampler = data_related.data.sampler.SubsetRandomSampler(indices)\n",
        "        data_related.data_loader_train.batch_sampler.sampler = new_sampler    \n",
        "        images,captions = next(iter(data_related.data_loader_train))    \n",
        "        images , captions = images.to(device) , captions.to(device)\n",
        "        encoder , decoder = encoder.to(device) , decoder.to(device)\n",
        "        encoder.zero_grad()    \n",
        "        decoder.zero_grad()\n",
        "        # todo: Compute output and loss.\n",
        "        #######################\n",
        "        output = decoder(encoder(images) ,captions)\n",
        "        loss = criterion(output.view(-1, vocab_size), captions.view(-1))\n",
        "        #######################\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        stat_vals = 'Epochs [%d/%d] Step [%d/%d] Loss [%.4f] ' %(e+1, num_epochs, step, total_step,loss.item())\n",
        "        if step % print_every == 0:\n",
        "            print(stat_vals)\n",
        "            sys.stdout.flush()\n",
        "            torch.save( encoder.state_dict() ,  os.path.join( model_save_path , 'encoderdata_{}.pkl'.format(e+1) ) )\n",
        "            torch.save( decoder.state_dict() ,  os.path.join( model_save_path , 'decoderdata_{}.pkl'.format(e+1) ) )\n",
        "    if e % save_every == 0:\n",
        "        torch.save( encoder.state_dict() ,  os.path.join( model_save_path , 'encoderdata_{}.pkl'.format(e+1) ) )\n",
        "        torch.save( decoder.state_dict() ,  os.path.join( model_save_path , 'decoderdata_{}.pkl'.format(e+1) ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqPh-8am4QJT"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu4_-lLu4QJT"
      },
      "outputs": [],
      "source": [
        "encoder.to(device) \n",
        "decoder.to(device)\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "original_img , processed_img  = next( data_related.data_iter )\n",
        "\n",
        "features = encoder(processed_img.to(device)).unsqueeze(1)\n",
        "final_output = decoder.generate( features  , max_len=20)\n",
        "data_related.get_sentences(original_img, final_output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Image Captioning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}